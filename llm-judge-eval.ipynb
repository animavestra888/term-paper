{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U -q google-genai transformers==4.51.3 accelerate==1.6.0 datasets==3.5.0 bitsandbytes==0.45.5 triton==3.2.0 unsloth==2025.3.19 torch==2.6.0 peft==0.15.2 trl==0.15.2 wandb==0.19.10","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] = ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Генерация ответов моделями","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport random\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset\nfrom transformers import BitsAndBytesConfig\nfrom peft import PeftModel\nimport wandb\nimport numpy as np\nfrom tqdm import tqdm\n\nSEED = 42\n\nNUM_SAMPLES = 500\nDATASET_NAME = \"HuggingFaceH4/ultrafeedback_binarized\"\nMODELS = {\n    \"BNF Model\": \"animavestra888-independent/Coursework/model-crmnp6jy:v24\",\n    \"DPO Model\": \"animavestra888-independent/Coursework/model-izl7baxm:v14\",\n    \"GRPO Model\": \"animavestra888-independent/Coursework/model-6enu8o1w:v7\",\n    \"Base Model\": \"Qwen/Qwen2.5-0.5B-Instruct\"\n}\nSYSTEM_PROMPT = \"You are a helpful AI assistant.\"\n\nbnb_cfg = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nGENERATION_CONFIG = {\n    \"max_new_tokens\": 1024,\n    \"temperature\": 0.7,\n    # \"top_p\": 0.9,\n    #\"do_sample\": False\n}\n\nOUTPUT_DIR = \"evaluation_results\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\ndef load_model(model_id, is_peft=False):\n    if is_peft:\n        wandb.init(project=\"Coursework\", entity=\"animavestra888-independent\", job_type=\"artifact_download\")\n\n        base_model, tokenizer = FastLanguageModel.from_pretrained(\n            \"Qwen/Qwen2.5-0.5B-Instruct\",\n            attn_implementation=\"flash_attention_2\",\n            quantization_config=bnb_cfg,\n            load_in_4bit=True,\n        )\n\n        try:\n            art = wandb.use_artifact(model_id, type=\"model\")\n            peft_dir = art.download()\n            model = PeftModel.from_pretrained(base_model, peft_dir)\n        finally:\n            wandb.finish()\n    else:\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            model_id,\n            attn_implementation=\"flash_attention_2\",\n            quantization_config=bnb_cfg,\n            load_in_4bit=True,\n        )\n\n    model.eval()\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    return model, tokenizer\n\ndef format_prompt(prompt, tokenizer):\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    return tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\ndef generate_answer(model, tokenizer, prompt):\n    formatted_prompt = format_prompt(prompt, tokenizer)\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True).to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=GENERATION_CONFIG[\"max_new_tokens\"],\n            do_sample=GENERATION_CONFIG[\"do_sample\"],\n            # temperature=GENERATION_CONFIG[\"temperature\"],\n            # top_p=GENERATION_CONFIG[\"top_p\"],\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n\n    return tokenizer.decode(\n        outputs[0][inputs.input_ids.shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n\n\ndef main():\n    print(\"Загрузка датасета...\")\n    dataset = load_dataset(DATASET_NAME, split=\"test_sft\")\n    all_indices = list(range(len(dataset)))\n    random.seed(SEED)\n    selected_indices = random.sample(all_indices, min(NUM_SAMPLES, len(dataset)))\n    dataset = dataset.select(selected_indices)\n    results_file = os.path.join(OUTPUT_DIR, f\"model_responses.csv\")\n\n\n    print(\"Загрузка моделей...\")\n    models_dict = {}\n    tokenizers = {}\n\n    for name, model_id in MODELS.items():\n        is_peft = name != \"Base Model\"\n        print(f\"Загрузка {name}...\")\n        models_dict[name], tokenizers[name] = load_model(model_id, is_peft)\n\n    print(\"\\nГенерация ответов...\")\n    results = []\n\n    for example in tqdm(dataset, desc=\"Генерация\"):\n        prompt = example[\"prompt\"]\n        responses = {}\n\n        for name, model in models_dict.items():\n            responses[name] = generate_answer(model, tokenizers[name], prompt)\n\n        results.append({\n            \"prompt\": prompt,\n            **responses\n        })\n\n    df = pd.DataFrame(results)\n\n    df.to_csv(results_file, index=False)\n    print(f\"Ответы моделей сохранены в: {results_file}\")\n\nmain()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Оценка ответов с помощью Gemini 2.5 Flash","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport random\nimport time\nimport re\nfrom collections import defaultdict\nfrom google import genai\nfrom google.genai import types\nimport wandb\nimport os\n\n\nSEED = 42\nrandom.seed(SEED)\n\nGOOGLE_API = \"\"\n\nWANDB_PROJECT = \"gemini-judge-evaluation\"\nWANDB_ENTITY = \"animavestra888-independent\"\nWANDB_NAME = \"gemini-llm-judge\"\nUSE_WANDB = True\n\nif USE_WANDB:\n    wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, name=WANDB_NAME)\n    wandb.define_metric(\"step\")\n    wandb.define_metric(\"evaluation/*\", step_metric=\"step\")\n    \n    wandb.config.update({\n        \"judge_model\": \"gemini-2.5-flash\",\n        \"temperature\": 0.0,\n    })\n\n\ndf = pd.read_csv('/kaggle/input/responses/model_responses.csv')\nvalid_answers = ['A', 'B', 'C', 'D']\nn_models = len(valid_answers)\nmodels = df.columns[1:].tolist() \nwins = {model_name: 0 for model_name in models}\nclient = genai.Client(api_key=GOOGLE_API)\nmax_retries = 5\n\nif USE_WANDB:\n    wandb_table = wandb.Table(columns=[\n        \"Prompt\", \n        \"Model A\", \"Response A\",\n        \"Model B\", \"Response B\",\n        \"Model C\", \"Response C\",\n        \"Model D\", \"Response D\",\n        \"Judge Response\", \"Verdict\", \"Winning Model\", \"Error\"\n    ])\n\nfor i, row in enumerate(df.iterrows()):\n    step = i + 1\n    _, data = row\n    prompt_text = data.iloc[0]  \n    \n    numbers = list(range(n_models))\n    random.shuffle(numbers)\n    answers = data.iloc[1:].tolist()\n    \n    eval_prompt = \"You are an expert evaluator. \" \\\n                  \"Compare responses based on helpfulness, accuracy, coherence, and alignment with human values. \" \\\n                  \"Respond ONLY with the letter of the best response (A, B, C, or D).\"\n    \n    model_responses = {}\n    for idx, num in enumerate(numbers):\n        label = valid_answers[idx]\n        model_responses[label] = answers[num]\n        eval_prompt += f\"\\nModel {label} answer: \\n{answers[num]}\\n\"\n    \n    response = None\n    error_msg = None\n    for attempt in range(max_retries):\n        try:\n            response = client.models.generate_content(\n                model=\"gemini-2.5-flash\",\n                contents=eval_prompt,\n                config=types.GenerateContentConfig(\n                    temperature=0.0\n                )\n            )\n            break\n        except Exception as e:\n            error_msg = str(e)\n            if attempt == max_retries - 1:\n                print(f\"Не удалось получить ответ после {max_retries} попыток: {error_msg}\")\n                verdict = \"ERROR\"\n                break\n            print(f\"Попытка запроса {attempt + 1} оказалась неудачной. Пробуем еще раз...\")\n            time.sleep(5)\n\n    verdict = \"UNKNOWN\"\n    winning_model = \"NONE\"\n    response_text = response.text if response else \"NO_RESPONSE\"\n    \n    if response is not None:\n        try:\n            if hasattr(response, 'text') and response.text is not None:\n                response_text = response.text\n\n                if response_text in valid_answers:\n                    verdict = response_text\n                else:\n                    boxed_match = re.search(r'\\\\boxed{([^{}]*)}', response_text)\n                    if not boxed_match:\n                        boxed_match = re.search(r'\\$?\\\\boxed{([^{}]*)}\\$?', response_text)\n                    \n                    if boxed_match:\n                        boxed_content = boxed_match.group(1).strip()\n                        for char in boxed_content:\n                            if char in valid_answers:\n                                verdict = char\n                                break\n        except Exception as e:\n            error_msg = f\"Ошибка парсинга ответа: {str(e)}\"\n            verdict = \"ERROR\"\n    \n    \n    if verdict in valid_answers:\n        model_index = valid_answers.index(verdict)\n        winning_model_index = numbers[model_index]\n        winning_model = models[winning_model_index]\n        wins[winning_model] += 1\n    \n    print(f\"\\n[{step}/{len(df)}] Ответ судьи: {response_text}\")\n    print(f\"Извлеченный вердикт судьи: {verdict} => {winning_model}\")\n    \n    if USE_WANDB:\n        wandb.log({\n            \"step\": step,\n            \"evaluation/response\": response_text,\n            \"evaluation/verdict\": verdict,\n            \"evaluation/winning_model\": winning_model,\n            \"evaluation/error\": error_msg or \"none\"\n        }, step=step)\n        \n        wandb_table.add_data(\n            prompt_text,\n            models[numbers[0]], model_responses.get('A', ''),\n            models[numbers[1]], model_responses.get('B', ''),\n            models[numbers[2]], model_responses.get('C', ''),\n            models[numbers[3]], model_responses.get('D', ''),\n            response_text,\n            verdict,\n            winning_model,\n            error_msg or \"none\"\n        )\n    \n    time.sleep(6) \n\nprint(\"\\nИтоговые результаты:\")\ntotal_evals = sum(wins.values())\nwin_rates = {}\nfor model, count in wins.items():\n    win_rate = (count / total_evals) * 100 if total_evals > 0 else 0\n    win_rates[model] = win_rate\n    print(f\"{model}: {count} wins ({win_rate:.2f}%)\")\n\nif USE_WANDB:\n    wandb.log({\n        \"evaluation/total_examples\": total_evals,\n    })\n    \n    for model, count in wins.items():\n        wandb.log({f\"evaluation/{model.replace(' ', '_')}_wins\": count})\n\n    summary_data = [[model, wins[model], win_rates[model]] for model in models]\n    summary_table = wandb.Table(\n        data=summary_data,\n        columns=[\"Model\", \"Wins\", \"Win Rate (%)\"]\n    )\n    \n    bar_chart = wandb.plot.bar(\n        summary_table,\n        \"Model\",\n        \"Wins\",\n        title=\"Model Win Counts\"\n    )\n    \n    wandb.log({\n        \"evaluation/detailed_results\": wandb_table,\n        \"evaluation/summary_table\": summary_table,\n        \"evaluation/win_comparison\": bar_chart,\n        **{f\"evaluation/win_rate_{model.replace(' ', '_')}\": rate for model, rate in win_rates.items()}\n    })\n    \n    results_df = pd.DataFrame({\n        \"Model\": models,\n        \"Wins\": [wins[model] for model in models],\n        \"WinRate\": [win_rates[model] for model in models]\n    })\n    results_df.to_csv(\"judge_results.csv\", index=False)\n    wandb.save(\"judge_results.csv\")\n    \n    wandb.finish()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}