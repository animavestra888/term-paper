{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U -q transformers==4.51.3 accelerate==1.6.0 datasets==3.5.0 bitsandbytes==0.45.5 triton==3.2.0 unsloth==2025.3.19 torch==2.6.0 peft==0.15.2 trl==0.15.2 wandb==0.19.10","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] = ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Генерация ответов моделями","metadata":{}},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport random\nimport pandas as pd\nimport torch\nfrom datasets import load_dataset\nfrom transformers import BitsAndBytesConfig\nfrom peft import PeftModel\nimport wandb\nimport numpy as np\nfrom tqdm import tqdm\n\nSEED = 42\n\nNUM_SAMPLES = 150\nDATASET_NAME = \"HuggingFaceH4/ultrafeedback_binarized\"\nMODELS = {\n    \"BNF Model\": \"animavestra888-independent/Coursework/model-crmnp6jy:v24\",\n    \"DPO Model\": \"animavestra888-independent/Coursework/model-izl7baxm:v14\",\n    \"GRPO Model\": \"animavestra888-independent/Coursework/model-6enu8o1w:v7\",\n    \"Base Model\": \"Qwen/Qwen2.5-0.5B-Instruct\"\n}\nSYSTEM_PROMPT = \"You are a helpful AI assistant.\"\n\nbnb_cfg = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nGENERATION_CONFIG = {\n    \"max_new_tokens\": 768,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"do_sample\": True\n}\n\nOUTPUT_DIR = \"evaluation_results\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\n\ndef load_model(model_id, is_peft=False):\n    if is_peft:\n        wandb.init(project=\"Coursework\", entity=\"animavestra888-independent\", job_type=\"artifact_download\")\n\n        base_model, tokenizer = FastLanguageModel.from_pretrained(\n            \"Qwen/Qwen2.5-0.5B-Instruct\",\n            attn_implementation=\"flash_attention_2\",\n            quantization_config=bnb_cfg,\n            load_in_4bit=True,\n        )\n\n        try:\n            art = wandb.use_artifact(model_id, type=\"model\")\n            peft_dir = art.download()\n            model = PeftModel.from_pretrained(base_model, peft_dir)\n        finally:\n            wandb.finish()\n    else:\n        model, tokenizer = FastLanguageModel.from_pretrained(\n            model_id,\n            attn_implementation=\"flash_attention_2\",\n            quantization_config=bnb_cfg,\n            load_in_4bit=True,\n        )\n\n    model.eval()\n    if tokenizer.pad_token_id is None:\n        tokenizer.pad_token_id = tokenizer.eos_token_id\n    return model, tokenizer\n\ndef format_prompt(prompt, tokenizer):\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    return tokenizer.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\ndef generate_answer(model, tokenizer, prompt):\n    formatted_prompt = format_prompt(prompt, tokenizer)\n    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True).to(model.device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=GENERATION_CONFIG[\"max_new_tokens\"],\n            do_sample=GENERATION_CONFIG[\"do_sample\"],\n            temperature=GENERATION_CONFIG[\"temperature\"],\n            top_p=GENERATION_CONFIG[\"top_p\"],\n            pad_token_id=tokenizer.pad_token_id,\n            eos_token_id=tokenizer.eos_token_id,\n        )\n\n    return tokenizer.decode(\n        outputs[0][inputs.input_ids.shape[1]:],\n        skip_special_tokens=True\n    ).strip()\n\n\ndef main():\n    print(\"Загрузка датасета...\")\n    dataset = load_dataset(DATASET_NAME, split=\"test_sft\")\n    all_indices = list(range(len(dataset)))\n    random.seed(SEED)\n    selected_indices = random.sample(all_indices, min(NUM_SAMPLES, len(dataset)))\n    dataset = dataset.select(selected_indices)\n    results_file = os.path.join(OUTPUT_DIR, f\"model_responses.csv\")\n\n\n    print(\"Загрузка моделей...\")\n    models_dict = {}\n    tokenizers = {}\n\n    for name, model_id in MODELS.items():\n        is_peft = name != \"Base Model\"\n        print(f\"Загрузка {name}...\")\n        models_dict[name], tokenizers[name] = load_model(model_id, is_peft)\n\n    print(\"\\nГенерация ответов...\")\n    results = []\n\n    for example in tqdm(dataset, desc=\"Генерация\"):\n        prompt = example[\"prompt\"]\n        responses = {}\n\n        for name, model in models_dict.items():\n            responses[name] = generate_answer(model, tokenizers[name], prompt)\n\n        results.append({\n            \"prompt\": prompt,\n            **responses\n        })\n\n    df = pd.DataFrame(results)\n\n    df.to_csv(results_file, index=False)\n    print(f\"Ответы моделей сохранены в: {results_file}\")\n\nmain()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Оценка ответов с помощью LLM","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport random\nfrom tqdm import tqdm\nimport torch\nfrom unsloth import FastLanguageModel\nfrom transformers import BitsAndBytesConfig\nimport wandb\nimport os\nimport numpy as np\n\nSEED = 42\nrandom.seed(SEED)\n\nWANDB_PROJECT = \"llm-evaluation\"\nWANDB_ENTITY = \"animavestra888-independent\"\nWANDB_NAME = \"llm-judge-evaluation\"\nUSE_WANDB = True\n\nMODELS = {\n    \"BNF Model\": \"animavestra888-independent/Coursework/model-crmnp6jy:v24\",\n    \"DPO Model\": \"animavestra888-independent/Coursework/model-izl7baxm:v14\",\n    \"GRPO Model\": \"animavestra888-independent/Coursework/model-6enu8o1w:v7\",\n    \"Base Model\": \"Qwen/Qwen2.5-0.5B-Instruct\"\n}\n\nif USE_WANDB:\n    wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, name=WANDB_NAME)\n    wandb.define_metric(\"step\")\n    wandb.define_metric(\"evaluation/*\", step_metric=\"step\")\n    wandb.config.update({\n        \"seed\": SEED,\n        \"judge_model\": \"Qwen/Qwen2.5-14B-Instruct\",\n        \"eval_models\": list(MODELS.keys()),\n        \"temperature\": 0,\n        \"max_new_tokens\": 10\n    })\n\nbnb_cfg = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nINPUT_FILE = \"/kaggle/input/responses/model_responses.csv\"\nOUTPUT_DIR = \"evaluation_results/\"\nos.makedirs(OUTPUT_DIR, exist_ok=True)\n\ndef load_judge_model():\n    print(\"Загрузка модели-судьи...\")\n    judge_model, judge_tokenizer = FastLanguageModel.from_pretrained(\n        \"Qwen/Qwen2.5-14B-Instruct\",\n        attn_implementation=\"flash_attention_2\",\n        quantization_config=bnb_cfg,\n        load_in_4bit=True,\n        max_seq_length=4096,\n    )\n    judge_model.eval()\n    if judge_tokenizer.pad_token_id is None:\n        judge_tokenizer.pad_token_id = judge_tokenizer.eos_token_id\n    return judge_model, judge_tokenizer\n\ndef format_judge_prompt(prompt, responses):\n    return [\n        {\n            \"role\": \"system\",\n            \"content\": (\n                \"You are an expert evaluator. Compare responses based on \"\n                \"helpfulness, accuracy, coherence, and alignment with human values. \"\n                \"Respond ONLY with the letter of the best response (A, B, C, or D).\"\n            )\n        },\n        {\n            \"role\": \"user\",\n            \"content\": (\n                f\"Prompt: {prompt}\\n\\n\" +\n                \"\\n\\n\".join(f\"Response {label}:\\n{resp}\" \n                          for label, resp in responses.items()) +\n                \"\\n\\nWhich response is best? Answer with A, B, C, or D only.\"\n            )\n        }\n    ]\n    \ndef run_llm_judge_evaluation(df):\n    print(\"\\n\" + \"=\"*50)\n    print(\"Запуск оценки LLM (4 модели)\")\n    print(\"=\"*50)\n    \n    labels = ['A', 'B', 'C', 'D']\n    for label in labels:\n        df[f\"model_{label}\"] = df.get(f\"model_{label}\", None)\n    \n    df[\"judge_choice\"] = df.get(\"judge_choice\", None)\n    df[\"judge_model\"] = df.get(\"judge_model\", None)\n    \n    judge_counts = {model: 0 for model in MODELS.keys()}\n    judge_model, judge_tokenizer = load_judge_model()\n    \n    if USE_WANDB:\n        wandb_table = wandb.Table(columns=[\n            \"Prompt\", \n            \"Model A\", \"Response A\", \n            \"Model B\", \"Response B\", \n            \"Model C\", \"Response C\", \n            \"Model D\", \"Response D\", \n            \"Judge Verdict\", \"Chosen Model\",\n        ])\n\n    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating\"):\n        prompt = row[\"prompt\"]\n        model_names = list(MODELS.keys())\n        random.shuffle(model_names)\n        label_map = {model: labels[i] for i, model in enumerate(model_names)}\n        responses = {label_map[model]: row[model] for model in model_names}\n        \n        for i, model in enumerate(model_names):\n            df.at[idx, f\"model_{labels[i]}\"] = model\n        \n        messages = format_judge_prompt(prompt, responses)\n        formatted_prompt = judge_tokenizer.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n        \n        inputs = judge_tokenizer(formatted_prompt, return_tensors=\"pt\").to(judge_model.device)\n        with torch.no_grad():\n            outputs = judge_model.generate(\n                **inputs,\n                max_new_tokens=10,\n                temperature=0,\n                do_sample=False,\n                pad_token_id=judge_tokenizer.pad_token_id,\n                eos_token_id=judge_tokenizer.eos_token_id,\n            )\n        \n        judge_response = judge_tokenizer.decode(\n            outputs[0][inputs.input_ids.shape[1]:], \n            skip_special_tokens=True\n        ).strip()\n\n        verdict = None\n        for char in judge_response:\n            if char in labels:\n                verdict = char\n                break\n\n        \n        chosen_model = None\n        if verdict:\n            for model, label in label_map.items():\n                if label == verdict:\n                    chosen_model = model\n                    judge_counts[chosen_model] += 1\n                    break\n\n        df.at[idx, \"judge_choice\"] = verdict\n        df.at[idx, \"judge_model\"] = chosen_model\n        \n        if wandb_table:\n            wandb_table.add_data(\n                prompt,\n                model_names[0], responses.get('A', ''),\n                model_names[1], responses.get('B', ''),\n                model_names[2], responses.get('C', ''),\n                model_names[3], responses.get('D', ''),\n                verdict or \"N/A\",\n                chosen_model or \"N/A\",\n            )\n    \n    total_evaluated = len(df) - df[\"judge_choice\"].isna().sum()\n    print(\"\\n\" + \"=\"*50)\n    print(\"Результаты\")\n    print(\"=\"*50)\n    \n    win_rates = {}\n    for model, count in judge_counts.items():\n        win_rates[model] = (count / total_evaluated * 100) if total_evaluated > 0 else 0\n        print(f\"{model}: {count} wins ({win_rates[model]:.1f}%)\")\n    if USE_WANDB:\n        summary_data = []\n        for model in MODELS.keys():\n            summary_data.append([\n                model, \n                judge_counts.get(model, 0),\n                win_rates.get(model, 0)\n            ])\n        \n        summary_table = wandb.Table(\n            data=summary_data,\n            columns=[\"Model\", \"Wins\", \"Win Rate (%)\"]\n        )\n        \n        comparison_table = wandb.Table(columns=[\"Model\", \"Wins\", \"Win Rate (%)\"])\n        for model in MODELS.keys():\n            comparison_table.add_data(\n                model,\n                judge_counts.get(model, 0),\n                f\"{win_rates.get(model, 0):.1f}%\"\n            )\n        \n        bar_chart = wandb.plot.bar(\n            comparison_table,\n            \"Model\",\n            \"Wins\",\n            title=\"Model Preferences\"\n        )\n        \n        wandb.log({\n            \"evaluation/summary\": summary_table,\n            \"evaluation/comparison_chart\": bar_chart,\n            \"evaluation/detailed_responses\": wandb_table,\n            \"evaluation/total_examples\": len(df),\n            \"evaluation/evaluated_examples\": total_evaluated,\n            **{f\"evaluation/win_rate_{model.replace(' ', '_')}\": win_rates.get(model, 0) \n               for model in MODELS.keys()}\n        })\n    \n    return df\n\n\ndef main():\n    df = pd.read_csv(INPUT_FILE)\n    df_results = run_llm_judge_evaluation(df)\n    \n    judge_file = os.path.join(OUTPUT_DIR, f\"judge_evaluation.csv\")\n    df_results.to_csv(judge_file, index=False)\n    print(f\"\\nРезультаты оценки LLM-судьей сохранены в: {judge_file}\")\n\n    if USE_WANDB:\n        wandb.finish()\n\nmain()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}