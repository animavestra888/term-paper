{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -U -q transformers==4.51.3 accelerate==1.6.0 datasets==3.5.0 bitsandbytes==0.45.5 triton==3.2.0 unsloth==2025.3.19 torch==2.6.0 peft==0.15.2 trl==0.15.2 wandb==0.19.10","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_API_KEY\"] =","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"reasoning_start = \"<reasoning>\"\nreasoning_end   = \"</reasoning>\"\nsolution_start  = \"<solution>\"\nsolution_end    = \"</solution>\"\n\nSYSTEM_GRPO_PROMPT = \\\nf\"\"\"You are given a problem.\nThink about the problem and provide your working out.\nPlace it between {reasoning_start} and {reasoning_end}.\nThen, provide your solution between {solution_start} and {solution_end}\"\"\"\nSYSTEM_PROMPT = \"You are a helpful AI assistant\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport os\nimport re\nimport torch\nfrom tqdm import tqdm\nfrom datasets import load_dataset\nfrom peft import PeftModel\nimport wandb\nfrom transformers import AutoTokenizer\nimport numpy as np\nfrom transformers import BitsAndBytesConfig\nfrom collections import defaultdict\n\nUSE_LLM_JUDGE = True\nJUDGE_MODEL_ID = \"Qwen/Qwen2.5-7B-instruct\" \n\nWANDB_PROJECT = \"gsm8k-evaluation\"\nWANDB_ENTITY = \"animavestra888-independent\"\nWANDB_NAME = \"gsm8k-grpo-model-llm-judge\"\nUSE_WANDB = True\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nMODELS_TO_TEST = [\n    # {\n    #     \"name\": \"Base Model\",\n    #     \"type\": \"base\",\n    #     \"model_id\": \"Qwen/Qwen2.5-0.5B-Instruct\"\n    # },\n    # {\n    #     \"name\": \"BNF Model\",\n    #     \"type\": \"peft\",\n    #     \"model_id\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n    #     \"peft_source\": \"wandb\",\n    #     \"wandb_artifact\": \"animavestra888-independent/Coursework/model-crmnp6jy:v24\"\n    # },\n    # {\n    #     \"name\": \"DPO Model\",\n    #     \"type\": \"peft\",\n    #     \"model_id\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n    #     \"peft_source\": \"wandb\",\n    #     \"wandb_artifact\": \"animavestra888-independent/Coursework/model-izl7baxm:v14\"\n    # },\n    {\n        \"name\": \"GRPO Model\",\n        \"type\": \"peft\",\n        \"model_id\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n        \"peft_source\": \"wandb\",\n        \"wandb_artifact\": \"animavestra888-independent/Coursework/model-6enu8o1w:v7\"\n    },\n]\n\nDATASET_NAME = \"gsm8k\"\nDATASET_SPLIT = \"main\"\nDATASET_SUBSET = \"test\"\n\nMAX_NEW_TOKENS = 1024\nNUM_BEAMS = 1\nDO_SAMPLE = False\nTEMPERATURE = 0.0\nLOG_INTERVAL = 50\n\nif USE_WANDB:\n    wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, name=WANDB_NAME)\n    wandb.define_metric(\"step\")\n    for model_config in MODELS_TO_TEST:\n        model_name = model_config[\"name\"]\n        wandb.define_metric(f\"{model_name}/accuracy\", step_metric=\"step\")\n        wandb.define_metric(f\"{model_name}/correct\", step_metric=\"step\")\n    wandb_table = wandb.Table(columns=[\"Model\", \"Question\", \"Generated\", \"Extracted\", \"Judge Verdict\", \"Ground Truth\", \"Correct\"])\n    wandb_intermediate_table = wandb.Table(columns=[\"Step\", \"Model\", \"Accuracy\", \"Examples Processed\"])\n    wandb.config.update({\n        \"dataset\": DATASET_NAME,\n        \"subset\": DATASET_SUBSET,\n        \"max_new_tokens\": MAX_NEW_TOKENS,\n        \"log_interval\": LOG_INTERVAL,\n        \"llm_judge\": USE_LLM_JUDGE,\n        \"judge_model\": JUDGE_MODEL_ID\n    })\n\n\ndataset = load_dataset(DATASET_NAME, DATASET_SPLIT, split=DATASET_SUBSET)\nprint(f\"Загружено примеров: {len(dataset)}\")\n\ndef extract_answer(completion):\n    boxed_match = re.search(r\"####\\s*([-]?[\\d,]+(?:\\.\\d+)?)\", completion)\n    if boxed_match:\n        return boxed_match.group(1).replace(',', '')\n    \n    numbers = re.findall(r\"[-]?(?:\\d{1,3}(?:,\\d{3})*|\\d+)(?:\\.\\d+)?\", completion)\n    if numbers:\n        return numbers[-1].replace(',', '')\n    \n    return None\n\ndef extract_ground_truth(answer_str):\n    match = re.search(r\"####\\s*([-]?[\\d,]+(?:\\.\\d+)?)\", answer_str)\n    if match:\n        return match.group(1).replace(',', '')\n    return None\n\nmodels = {}\ntokenizers = {}\njudge_model = None\njudge_tokenizer = None\n\nfor config in MODELS_TO_TEST:\n    model_name = config[\"name\"]\n    \n    try:\n        if config[\"type\"] == \"base\":\n            model, tokenizer = FastLanguageModel.from_pretrained(\n                config[\"model_id\"],\n                attn_implementation=\"flash_attention_2\",\n                quantization_config=bnb_config,\n            )\n        elif config[\"type\"] == \"peft\":\n            base_model, tokenizer = FastLanguageModel.from_pretrained(\n                config[\"model_id\"],\n                attn_implementation=\"flash_attention_2\",\n                quantization_config=bnb_config,\n                load_in_4bit=True\n            )\n            \n            if config[\"peft_source\"] == \"wandb\":\n                artifact = wandb.use_artifact(config[\"wandb_artifact\"], type='model')\n                peft_path = artifact.download()\n                model = PeftModel.from_pretrained(base_model, peft_path)\n        \n        model.eval()\n        models[model_name] = model\n        tokenizers[model_name] = tokenizer\n        \n        if tokenizer.pad_token_id is None:\n            tokenizer.pad_token_id = tokenizer.eos_token_id\n            \n        print(f\"    Модель '{model_name}' успешно загружена\\n\")\n        \n    except Exception as e:\n        print(f\"    Ошибка загрузки модели '{model_name}': {str(e)}\\n\")\n\nif USE_LLM_JUDGE:\n    print(\"Загрузка модели-судьи...\")\n    try:\n        judge_model, judge_tokenizer = FastLanguageModel.from_pretrained(\n            JUDGE_MODEL_ID,\n            attn_implementation=\"flash_attention_2\",\n            quantization_config=bnb_config,\n            load_in_4bit=True\n        )\n        judge_model.eval()\n        if judge_tokenizer.pad_token_id is None:\n            judge_tokenizer.pad_token_id = judge_tokenizer.eos_token_id\n        print(f\"    Модель-судья '{JUDGE_MODEL_ID}' успешно загружена\\n\")\n    except Exception as e:\n        print(f\"    Ошибка загрузки модели-судии: {str(e)}\")\n        USE_LLM_JUDGE = False\n\ndef verify_with_judge(question, generated_response, ground_truth):\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are an expert math evaluator. Determine if the candidate's answer matches the ground truth. Consider only the final numerical answer.\"},\n        {\"role\": \"user\", \"content\": f\"Problem: {question}\\nCandidate Answer: {generated_response}\\nGround Truth: {ground_truth}\\n\\nIs the candidate's answer correct? Respond ONLY with 'yes' or 'no'.\"}\n    ]\n    \n    prompt = judge_tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n    )\n    \n    inputs = judge_tokenizer(prompt, return_tensors=\"pt\").to(judge_model.device)\n    \n    with torch.no_grad():\n        outputs = judge_model.generate(\n            **inputs,\n            max_new_tokens=10,\n            temperature=0.0,\n            do_sample=False,\n            pad_token_id=judge_tokenizer.pad_token_id,\n            eos_token_id=judge_tokenizer.eos_token_id,\n        )\n    \n    response = judge_tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n    return \"yes\" in response.lower()\n\ndef format_prompt(question, tokenizer):\n    messages = [\n        {\"role\": \"system\", \"content\": SYSTEM_GRPO_PROMPT},\n        {\"role\": \"user\", \"content\": question}\n    ]\n    return tokenizer.apply_chat_template(\n        messages, \n        tokenize=False, \n        add_generation_prompt=True\n    )\n\nresults = {model_name: {\"correct\": 0, \"total\": 0, \"predictions\": [], \"judge_used\": 0, \"judge_correct\": 0} for model_name in models}\n\nfor i, example in enumerate(tqdm(dataset, desc=\"Обработка примеров\")):\n    question = example['question']\n    ground_truth = extract_ground_truth(example['answer'])\n    \n    if not ground_truth:\n        continue\n    \n    try:\n        gt_number = float(ground_truth)\n    except ValueError:\n        continue\n\n    for model_name, model in models.items():\n        tokenizer = tokenizers[model_name]\n        prompt = format_prompt(question, tokenizer)\n        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n        \n        results[model_name][\"total\"] += 1\n        \n        with torch.no_grad():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                num_beams=NUM_BEAMS,\n                do_sample=DO_SAMPLE,\n                temperature=TEMPERATURE,\n                pad_token_id=tokenizer.pad_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n            )\n        \n        completion = tokenizer.decode(\n            outputs[0][inputs.input_ids.shape[1]:], \n            skip_special_tokens=True\n        )\n        \n        extracted_ans = extract_answer(completion)\n        is_correct = False\n        judge_verdict = \"N/A\"\n        used_judge = False\n        \n        if extracted_ans:\n            try:\n                ans_number = float(extracted_ans)\n                if ans_number == gt_number:\n                    is_correct = True\n            except ValueError:\n                pass\n        \n        if not is_correct and USE_LLM_JUDGE and judge_model:\n            used_judge = True\n            results[model_name][\"judge_used\"] += 1\n            if verify_with_judge(question, completion, ground_truth):\n                is_correct = True\n                results[model_name][\"judge_correct\"] += 1\n                judge_verdict = \"Correct\"\n            else:\n                judge_verdict = \"Incorrect\"\n        \n        if is_correct:\n            results[model_name][\"correct\"] += 1\n\n        results[model_name][\"predictions\"].append({\n            \"question\": question,\n            \"generated\": completion,\n            \"extracted\": extracted_ans,\n            \"judge_verdict\": judge_verdict,\n            \"ground_truth\": ground_truth,\n            \"correct\": is_correct,\n            \"used_judge\": used_judge\n        })\n        \n        if USE_WANDB:\n            wandb_table.add_data(\n                model_name,\n                question,\n                completion,\n                extracted_ans,\n                judge_verdict,\n                ground_truth,\n                \"correct\" if is_correct else \"incorrect\"\n            )\n\n    current_step = i + 1\n    \n    if (i+1) % LOG_INTERVAL == 0 or (i+1) == len(dataset):\n        print(f\"\\n--- Отчет после {i+1} примеров ---\")\n        \n        for model_name in models:\n            total = results[model_name][\"total\"]\n            correct = results[model_name][\"correct\"]\n            accuracy = correct / total * 100 if total > 0 else 0\n            \n            print(f\"{model_name}:\")\n            print(f\"  Правильных: {correct}/{total}\")\n            print(f\"  Точность: {accuracy:.2f}%\")\n            \n            if USE_LLM_JUDGE:\n                judge_used = results[model_name][\"judge_used\"]\n                judge_correct = results[model_name][\"judge_correct\"]\n                print(f\"  Проверок судьей: {judge_used} ({judge_correct} правильных)\")\n            \n            if USE_WANDB:\n                wandb.log({\n                    \"step\": current_step,\n                    f\"{model_name}/accuracy\": accuracy,\n                    f\"{model_name}/correct\": correct,\n                    f\"{model_name}/total\": total\n                })\n\nif USE_WANDB:\n    final_accuracy_data = [[model_name, results[model_name][\"correct\"] / results[model_name][\"total\"] * 100] \n                           for model_name in models]\n    bar_chart = wandb.plot.bar(\n        wandb.Table(\n            data=final_accuracy_data,\n            columns=[\"Model\", \"Accuracy\"]\n        ),\n        \"Model\",\n        \"Accuracy\",\n        title=\"Final Accuracy\"\n    )\n\n    wandb.log({\n        \"final_accuracy_chart\": bar_chart,\n        \"predictions_table\": wandb_table,\n        **{f\"final_accuracy/{model}\": results[model][\"correct\"] / results[model][\"total\"] * 100 \n           for model in models}\n    })\n    wandb.finish()\n\nprint(\"\\nТестирование завершено\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}