{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install -q pip3-autoremove\n# !pip install -q torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n# !pip install -q unsloth vllm","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport wandb\nos.environ[\"WANDB_API_KEY\"] = \nos.environ[\"WANDB_PROJECT\"] = \"Coursework\" \nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import unsloth\nfrom unsloth import FastLanguageModel\nimport torch\nfrom datasets import load_dataset\nfrom trl import DPOTrainer, DPOConfig\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    DataCollatorForSeq2Seq\n)\nfrom peft import LoraConfig\nfrom transformers import BitsAndBytesConfig\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, \n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n)\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    \"Qwen/Qwen2.5-0.5B-Instruct\",\n    quantization_config=bnb_config,\n    fast_inference=True\n)\n\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=16,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0,\n    bias=\"none\",\n    use_gradient_checkpointing=True,\n    random_state=42,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\nreasoning_start = \"<reasoning>\"\nreasoning_end   = \"</reasoning>\" \nsolution_start  = \"<solution>\"\nsolution_end    = \"</solution>\"\n\nSYSTEM_PROMPT = \\\nf\"\"\"You are given a problem.\nThink about the problem and provide your working out.\nPlace it between {reasoning_start} and {reasoning_end}.\nThen, provide your solution between {solution_start} and {solution_end}\"\"\"\n\ndef extract_solution(text):\n    if solution_start in text and solution_end in text:\n        solution = text.split(solution_start)[-1]\n        solution = solution.split(solution_end)[0]\n        return solution.strip()\n    return \"\"\n\ndef extract_hash_answer(text):\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip()\n\ndef correctness_reward_func(prompts, completions, answer, **kwargs):\n    responses = [completion[0]['content'] for completion in completions]\n    q = prompts[0][-1]['content']\n    extracted_responses = [extract_solution(r) for r in responses]\n    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", \n          f\"\\nResponse:\\n{responses[0]}\", f\"\\nExtracted:\\n{extracted_responses[0]}\")\n    return [2.0 if r == a else 0.0 for r, a in zip(extracted_responses, answer)]\n\ndef int_reward_func(completions, **kwargs):\n    responses = [completion[0]['content'] for completion in completions]\n    extracted_responses = [extract_solution(r) for r in responses]\n    return [0.5 if r.isdigit() else 0.0 for r in extracted_responses]\n\ndef strict_format_reward_func(completions, **kwargs):\n    pattern = rf\"^{re.escape(reasoning_start)}[\\s\\S]*?{re.escape(reasoning_end)}\\s*{re.escape(solution_start)}[\\s\\S]*?{re.escape(solution_end)}$\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.search(pattern, r, re.DOTALL) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef soft_format_reward_func(completions, **kwargs)]:\n    pattern = rf\"{re.escape(reasoning_start)}[\\s\\S]*?{re.escape(reasoning_end)}\\s*{re.escape(solution_start)}[\\s\\S]*?{re.escape(solution_end)}\"\n    responses = [completion[0][\"content\"] for completion in completions]\n    matches = [re.search(pattern, r, re.DOTALL) for r in responses]\n    return [0.5 if match else 0.0 for match in matches]\n\ndef reasoning_length_reward_func(completions, **kwargs):\n    responses = [completion[0][\"content\"] for completion in completions]\n    rewards = []\n    \n    for resp in responses:\n        if reasoning_start in resp and reasoning_end in resp:\n            reason_text = resp.split(reasoning_start)[1].split(reasoning_end)[0].strip()\n        else:\n            reason_text = \"\"\n\n        reason_length = len(reason_text)\n        min_meaningful_length = 100 \n        if reason_length < min_meaningful_length:\n            reward = reason_length * 0.5 / min_meaningful_length  - 0.5\n        else:\n            reward = 0.125\n            \n        rewards.append(reward)\n    return rewards\n\n\ndef count_xml(text):\n    count = 0.0\n    if reasoning_start in text:\n        count += 0.125\n        if reasoning_end in text:\n            count += 0.125\n            post_reasoning = text.split(reasoning_end)[-1]\n            if post_reasoning.strip() and not post_reasoning.strip().startswith(solution_start):\n                count -= len(post_reasoning) * 0.001\n    \n    if solution_start in text:\n        count += 0.125\n        if solution_end in text:\n            count += 0.125\n            post_solution = text.split(solution_end)[-1]\n            if post_solution.strip():\n                count -= len(post_solution) * 0.001\n    return count\n\ndef xmlcount_reward_func(completions, **kwargs):\n    contents = [completion[0][\"content\"] for completion in completions]\n    return [count_xml(c) for c in contents]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset, Dataset\n\ndef get_gsm8k_questions(split = \"train\"):\n    data = load_dataset('openai/gsm8k', 'main')[split]\n    data = data.map(lambda x: {\n        'prompt': [\n            {'role': 'system', 'content': SYSTEM_PROMPT},\n            {'role': 'user', 'content': x['question']}\n        ],\n        'answer': extract_hash_answer(x['answer'])\n    })\n    return data","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset = get_gsm8k_questions(split='train')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import GRPOConfig, GRPOTrainer\n\nmax_prompt_length = 256\nmax_seq_length = 1024\n\ntraining_args = GRPOConfig(\n    use_vllm=True,\n    beta=0.05,\n    learning_rate = 5e-6,\n    weight_decay = 0.1,\n    warmup_ratio = 0.1,\n    lr_scheduler_type = \"cosine\",\n    optim = \"adamw_8bit\",\n    logging_steps = 1,\n    per_device_train_batch_size = 24,\n    gradient_accumulation_steps = 1, \n    num_generations = 8,\n    max_prompt_length = max_prompt_length,\n    max_completion_length = max_seq_length - max_prompt_length,\n    num_train_epochs = 1, \n    max_grad_norm = 0.1,\n    report_to = \"wandb\",\n    output_dir = \"Qwen2.5-0.5B-Instruct-GRPO\",\n    gradient_checkpointing=True,\n    save_strategy='steps',\n    save_steps=250,\n    push_to_hub=True,\n    hub_model_id='theevolutionisnear/Qwen2.5-0.5B-Instruct-GRPO',\n    hub_strategy='checkpoint',\n    hub_token=True,\n    fp16=True,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WANDB_PROJECT = \"Coursework\"\nWANDB_ENTITY = \"animavestra888-independent\"\nWANDB_NAME = \"Qwen2.5-0.5B-Instruct-GRPO\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.init(project=WANDB_PROJECT, entity=WANDB_ENTITY, name=WANDB_NAME, settings=wandb.Settings(init_timeout=300))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from huggingface_hub import snapshot_download\n\n# root_dir = snapshot_download(\n#     repo_id=\"theevolutionisnear/Qwen2.5-0.5B-Instruct-GRPO\",\n#     revision=\"0ffec180501348783f125b8558475e61c17809c7\",)\n\n# ckpt_dir = f\"{root_dir}/last-checkpoint\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# _torch_load = torch.load\n\n# def _load_with_full_pickle(*args, **kwargs):\n#     kwargs[\"weights_only\"] = False\n\n#     return _torch_load(*args, **kwargs)\n    \n# torch.load = _load_with_full_pickle \n\n# wandb.init(project=\"Coursework\",\n#            id=\"6enu8o1w\",\n#            resume=\"must\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer = GRPOTrainer(\n    model = model,\n    processing_class = tokenizer,\n    reward_funcs = [\n        xmlcount_reward_func,\n        soft_format_reward_func,\n        strict_format_reward_func,\n        int_reward_func,\n        correctness_reward_func,\n        reasoning_length_reward_func\n    ],\n    args = training_args,\n    train_dataset = dataset,\n)\ntrainer.train()\n#trainer.train(resume_from_checkpoint=ckpt_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}